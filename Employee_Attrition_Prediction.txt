# ðŸ“Œ Step 1: Import Required Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import shap
import warnings

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LinearRegression
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    accuracy_score,
    ConfusionMatrixDisplay,
    mean_squared_error,
    r2_score
)

warnings.filterwarnings("ignore")
sns.set_style("whitegrid")
shap.initjs()

# ðŸ“Œ Step 2: Create Synthetic Employee Dataset
np.random.seed(42)
N = 150

data = pd.DataFrame({
    'Attendance (%)': np.random.randint(50, 100, N),
    'Project Delivery Score': np.random.randint(50, 100, N),
    'Work Hours/Week': np.random.randint(30, 60, N),
    'Previous Rating': np.random.randint(2, 5, N),
    'Stress Level': np.random.randint(1, 10, N),
    'Focus Level': np.random.randint(1, 10, N),
    'Sleep Hours': np.random.randint(4, 9, N),
    'Education Level': np.random.choice(['Diploma', 'Bachelor', 'Master'], N),
    'Remote Work': np.random.choice(['Yes', 'No'], N),
    'Gender': np.random.choice(['Male', 'Female'], N),
    'Training Completed': np.random.choice(['Yes', 'No'], N)
})

# ðŸ“Œ Step 3: Add Target Columns
data['Performance Score'] = (
    0.25 * data['Previous Rating'] * 20 +
    0.25 * data['Project Delivery Score'] +
    0.15 * data['Attendance (%)'] +
    0.1 * data['Work Hours/Week'] -
    0.1 * data['Stress Level'] +
    0.15 * data['Focus Level'] * 10
) + np.random.normal(0, 10, N)

data['Attrition Risk'] = (data['Performance Score'] < 140).astype(int)

data.loc[data['Attrition Risk'] == 1, 'Stress Level'] += np.random.randint(1, 3, size=data['Attrition Risk'].sum())
data.loc[data['Attrition Risk'] == 1, 'Sleep Hours'] -= np.random.randint(1, 2, size=data['Attrition Risk'].sum())
data['Stress Level'] = data['Stress Level'].clip(1, 10)
data['Sleep Hours'] = data['Sleep Hours'].clip(4, 9)

# ðŸ“Œ Step 4: Encode Categorical Features
df = pd.get_dummies(data, drop_first=True)

# ðŸ“Œ Step 5: Split Features and Targets
X = df.drop(['Performance Score', 'Attrition Risk'], axis=1)
y_reg = df['Performance Score']
y_clf = df['Attrition Risk']

# ðŸ“Œ Step 6: Feature Scaling
scaler = StandardScaler()
X_scaled_array = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled_array, columns=X.columns)

# ðŸ“Œ Step 7: Train-Test Split
X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_scaled, y_reg, test_size=0.2, random_state=42)
X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_scaled, y_clf, test_size=0.2, random_state=42)

# ðŸ“Œ Step 8: Train Regression Model for Performance
reg_model = LinearRegression()
reg_model.fit(X_train_r, y_train_r)
y_pred_r = reg_model.predict(X_test_r)

print("ðŸŽ¯ Performance Score Prediction")
print(f"Mean Squared Error: {mean_squared_error(y_test_r, y_pred_r):.2f}")
print(f"RÂ² Score: {r2_score(y_test_r, y_pred_r):.2f}")

# ðŸ“Œ Step 9: Plot Actual vs Predicted
plt.figure(figsize=(8, 5))
sns.scatterplot(x=y_test_r, y=y_pred_r, color='blue', s=80, edgecolor='black')
plt.plot([y_test_r.min(), y_test_r.max()], [y_test_r.min(), y_test_r.max()], 'r--', lw=2)
plt.title("Actual vs Predicted: Employee Performance", fontsize=14, fontweight='bold')
plt.xlabel("Actual Score")
plt.ylabel("Predicted Score")
plt.tight_layout()
plt.show()

# ðŸ“Œ Step 10: Train Classifier for Attrition Risk
clf_model = RandomForestClassifier(n_estimators=100, random_state=42)
clf_model.fit(X_train_c, y_train_c)
y_pred_c = clf_model.predict(X_test_c)

print("\\nðŸ“Š Attrition Classification Report")
print(classification_report(y_test_c, y_pred_c))

# ðŸ“Œ Step 11: Confusion Matrix
conf_mat = confusion_matrix(y_test_c, y_pred_c, labels=[0, 1])
disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat, display_labels=["No Attrition", "Attrition"])
fig, ax = plt.subplots(figsize=(6, 4))
disp.plot(cmap="Purples", ax=ax, values_format='d')
plt.title("Confusion Matrix: Attrition Prediction", fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# ðŸ“Œ Step 12: SHAP Feature Interpretability
clf_final = RandomForestClassifier(n_estimators=100, random_state=42)
clf_final.fit(X_scaled, y_clf)
explainer = shap.Explainer(clf_final, X_scaled)
shap_values = explainer(X_scaled)

# ðŸ“Œ Step 13: SHAP Summary Plot
shap.plots.beeswarm(shap_values, max_display=10)


